{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### NLP & Data Mining Endterm\n",
        "\n",
        "### Divina Komal Dcunha | 2021NOVVUGP0016\n",
        "\n",
        "In this notebook, a single-layer transformer encoder was implemented from scratch and trained on the IMDB dataset.\n",
        "\n",
        "References were taken from the following link: https://github.com/HosseinZaredar/Transformer-from-Scratch?tab=readme-ov-file\n",
        "\n",
        "\n",
        "The architecture includes:\n",
        "1. Multi-Head Self-Attention: Captures relationships between words in the input sequence.\n",
        "2. Feedforward Network: Enhances representation power after self-attention.\n",
        "3. Embedding Layer: Input text is embedded into dense vector representations, crucial for capturing semantic meaning.\n",
        "4. Positional Encoding: Adds positional information to the embeddings, helping the model understand word order.\n"
      ],
      "metadata": {
        "id": "sjDYTZn9H3xG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Future Direction: Transferring onto RasPi?"
      ],
      "metadata": {
        "id": "OlUp00dZenjH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW98S6YIJZ8c"
      },
      "outputs": [],
      "source": [
        "import torch as T\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "#Prepare and load the IMDB dataset via tfds.load()\n",
        "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised=True)\n",
        "train_data, test_data = imdb['train'], imdb['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset size via metadata 'info'\n",
        "print(\"Total number of examples in the dataset:\", info.splits.total_num_examples)\n",
        "print(\"Number of training examples:\", info.splits['train'].num_examples)\n",
        "print(\"Number of testing examples:\", info.splits['test'].num_examples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mU_upQkToiUp",
        "outputId": "7d311e11-21fc-424a-e8c4-1bb86bf033be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of examples in the dataset: 100000\n",
            "Number of training examples: 25000\n",
            "Number of testing examples: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(info.splits)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "widm1qhts29Z",
        "outputId": "d9deefa8-9423-4852-b2b2-b670e951228a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'train': <SplitInfo num_examples=25000, num_shards=1>, 'test': <SplitInfo num_examples=25000, num_shards=1>, 'unsupervised': <SplitInfo num_examples=50000, num_shards=1>}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "50,000 examples (unsupervised data, typically used for tasks like unsupervised pretraining or testing models without labels); reviews but without labels (i.e., no sentiment classification provided)."
      ],
      "metadata": {
        "id": "ot75oOLtszvH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# Count positive and negative examples\n",
        "train_labels = Counter(label.numpy() for _, label in train_data) #Ignoring review (review,label) -> (_, label)\n",
        "test_labels = Counter(label.numpy() for _, label in test_data)\n",
        "\n",
        "print(\"Training Data:\")\n",
        "print(f\"  Positive examples: {train_labels[1]}\")\n",
        "print(f\"  Negative examples: {train_labels[0]}\")\n",
        "\n",
        "print(\"\\nTesting Data:\")\n",
        "print(f\"  Positive examples: {test_labels[1]}\")\n",
        "print(f\"  Negative examples: {test_labels[0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvVzV1grolO1",
        "outputId": "f336925a-232f-45f0-f8d3-fed7182719b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "  Positive examples: 12500\n",
            "  Negative examples: 12500\n",
            "\n",
            "Testing Data:\n",
            "  Positive examples: 12500\n",
            "  Negative examples: 12500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for example, label in train_data.take(5):  # Take 5 examples\n",
        "    print(\"Review:\", example.numpy().decode('utf-8'))  # Decode byte strings\n",
        "    print(\"Label:\", label.numpy())  # 0 or 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6M9EN1-oMx1",
        "outputId": "ff1cf7ef-d402-4ab8-9ad2-173b7b19a0a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\n",
            "Label: 0\n",
            "Review: I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all.\n",
            "Label: 0\n",
            "Review: Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust.\n",
            "Label: 0\n",
            "Review: This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received.\n",
            "Label: 1\n",
            "Review: As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love.\n",
            "Label: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fdpdh4o8JdZ0"
      },
      "outputs": [],
      "source": [
        "# Step 1: Build Vocabulary\n",
        "# Function to build vocabulary from the dataset using a tokenizer\n",
        "# The vocabulary assigns a unique index to each token, reserving 0 for padding\n",
        "def build_vocab(dataset, tokenizer):\n",
        "    vocab = {}\n",
        "    index = 1  # Starting index for vocabulary, reserve 0 for padding\n",
        "    for text, _ in tfds.as_numpy(dataset):  # Convert dataset to NumPy format\n",
        "        tokens = tokenizer(text.decode('utf-8'))  # Tokenize the text\n",
        "        for token in tokens:  # Iterate through tokens\n",
        "            if token not in vocab:  # If token not already in vocabulary\n",
        "                vocab[token] = index  # Assign a unique index\n",
        "                index += 1\n",
        "    return vocab  # Return the constructed vocabulary\n",
        "\n",
        "# Simple tokenizer function\n",
        "# This function tokenizes text by splitting it on whitespace\n",
        "def simple_tokenizer(text):\n",
        "    return text.split()  # Tokenize by whitespace for simplicity\n",
        "\n",
        "# Build vocab from training data\n",
        "# Using the build_vocab function and simple_tokenizer to create a vocabulary\n",
        "vocab = build_vocab(train_data, simple_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxsmNkpLJip3"
      },
      "outputs": [],
      "source": [
        "# Step 2: Tokenizer with Vocabulary Closure\n",
        "# Function to create a tokenizer using the given vocabulary\n",
        "# Unknown words are mapped to index 0\n",
        "def get_tokenizer(vocab):\n",
        "    def tokenizer(text):\n",
        "        return [vocab.get(word, 0) for word in text.split()]  # Use 0 for unknown words\n",
        "    return tokenizer\n",
        "\n",
        "# Tokenizer instance created using the vocabulary\n",
        "tokenizer = get_tokenizer(vocab)\n",
        "\n",
        "# Custom Dataset Class\n",
        "# A PyTorch Dataset class for handling the IMDB dataset\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self, tf_dataset, tokenizer, max_length=256):\n",
        "        self.data = list(tfds.as_numpy(tf_dataset))  # Convert to NumPy format\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)  # Number of samples in the dataset\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, label = self.data[idx]  # Extract text and label\n",
        "        tokens = self.tokenizer(text.decode('utf-8'))  # Tokenize the text\n",
        "        # Pad or truncate tokens to the maximum length\n",
        "        padded_tokens = tokens[:self.max_length] + [0] * max(0, self.max_length - len(tokens))\n",
        "        return T.tensor(padded_tokens), T.tensor(label, dtype=T.long)\n",
        "\n",
        "# Transformer Components (Embedding, Attention, Encoder)\n",
        "# Embedding layer: Combines word and positional embeddings\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, vocab_size, max_length, embed_dim, dropout=0.1):\n",
        "        super(Embedding, self).__init__()\n",
        "        self.word_embed = nn.Embedding(vocab_size, embed_dim)  # Word embeddings\n",
        "        self.pos_embed = nn.Embedding(max_length, embed_dim)  # Positional embeddings\n",
        "        self.dropout = nn.Dropout(dropout)  # Dropout for regularization\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_length = x.shape\n",
        "        device = T.device('cuda' if T.cuda.is_available() else 'cpu')\n",
        "        # Generate position indices for the sequence\n",
        "        positions = T.arange(0, seq_length).expand(batch_size, seq_length).to(device)\n",
        "        # Combine word and positional embeddings\n",
        "        embedding = self.word_embed(x) + self.pos_embed(positions)\n",
        "        return self.dropout(embedding)\n",
        "\n",
        "# Multi-Head Self-Attention mechanism\n",
        "class MHSelfAttention(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MHSelfAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads  # Dimension of each attention head\n",
        "        assert (self.num_heads * self.head_dim == self.embed_dim), \\\n",
        "            'Embed size must be divisible by the number of heads'\n",
        "        # Linear layers for queries, keys, and values\n",
        "        self.w_queries = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "        self.w_keys = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "        self.w_values = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(self.head_dim * self.num_heads, self.embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.shape[0]\n",
        "        sentence_len = x.shape[1]\n",
        "        # Compute queries, keys, and values for attention\n",
        "        queries = self.w_queries(x).reshape(batch_size, sentence_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        keys = self.w_keys(x).reshape(batch_size, sentence_len, self.num_heads, self.head_dim).permute(0, 2, 3, 1)\n",
        "        values = self.w_values(x).reshape(batch_size, sentence_len, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        # Compute scaled dot-product attention scores\n",
        "        attention_scores = T.einsum('bijk,bikl->bijl', queries, keys)\n",
        "        attention_dist = T.softmax(attention_scores / (self.embed_dim ** (1/2)), dim=-1)\n",
        "        # Apply attention distribution to values\n",
        "        attention_out = T.einsum('bijk,bikl->bijl', attention_dist, values)\n",
        "        # Concatenate heads and pass through output linear layer\n",
        "        concatenated_out = attention_out.permute(0, 2, 1, 3).reshape(batch_size, sentence_len, self.embed_dim)\n",
        "        return concatenated_out\n",
        "\n",
        "# Transformer Encoder block\n",
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, forward_expansion, dropout=0.1):\n",
        "        super(TransformerEncoder, self).__init__()\n",
        "        self.attention = MHSelfAttention(embed_dim, num_heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_dim, forward_expansion * embed_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(forward_expansion * embed_dim, embed_dim)\n",
        "        )\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply multi-head self-attention and residual connection\n",
        "        attention_out = self.dropout(self.attention(x))\n",
        "        x = self.norm1(x + attention_out)\n",
        "        # Apply feed-forward network and residual connection\n",
        "        forward_out = self.dropout(self.feed_forward(x))\n",
        "        return self.norm2(x + forward_out)\n",
        "\n",
        "# Transformer Classifier Model\n",
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size, max_length, embed_dim, num_heads, forward_expansion, num_layers, output_dim):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedder = Embedding(vocab_size, max_length, embed_dim)  # Embedding layer\n",
        "        self.encoder = nn.ModuleList(\n",
        "            [TransformerEncoder(embed_dim, num_heads, forward_expansion) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_dim, output_dim)  # Fully connected output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedder(x)  # Pass through embedding layer\n",
        "        for layer in self.encoder:  # Pass through encoder layers\n",
        "            x = layer(x)\n",
        "        x = x.mean(dim=1)  # Global average pooling\n",
        "        return self.fc(x)  # Output layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKgxNYa9DtbZ"
      },
      "outputs": [],
      "source": [
        "# Model and Training Parameters\n",
        "# Defining key parameters for the Transformer model and training\n",
        "VOCAB_SIZE = len(vocab) + 1  # Vocabulary size (+1 for the padding token)\n",
        "EMBED_DIM = 128  # Dimensionality of embedding vectors\n",
        "NUM_HEADS = 8  # Number of attention heads\n",
        "FORWARD_EXPANSION = 4  # Expansion factor for the feed-forward network\n",
        "NUM_LAYERS = 4  # Number of encoder layers\n",
        "OUTPUT_DIM = 2  # Output dimensions (binary classification: positive/negative sentiment)\n",
        "MAX_LENGTH = 256  # Maximum sequence length\n",
        "BATCH_SIZE = 32  # Batch size for training\n",
        "LEARNING_RATE = 0.001  # Learning rate for optimizer\n",
        "\n",
        "# DataLoaders\n",
        "# Convert datasets into PyTorch DataLoader for batching and shuffling\n",
        "train_dataset = IMDBDataset(train_data, tokenizer, max_length=MAX_LENGTH)  # Training dataset\n",
        "test_dataset = IMDBDataset(test_data, tokenizer, max_length=MAX_LENGTH)  # Test dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)  # DataLoader for training\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)  # DataLoader for testing\n",
        "\n",
        "# Training Loop Setup\n",
        "# Set up device, model, loss function, and optimizer\n",
        "device = T.device('cuda' if T.cuda.is_available() else 'cpu')  # Use GPU if available\n",
        "# Initialize Transformer model\n",
        "model = TransformerClassifier(VOCAB_SIZE, MAX_LENGTH, EMBED_DIM, NUM_HEADS, FORWARD_EXPANSION, NUM_LAYERS, OUTPUT_DIM)\n",
        "model.to(device)  # Move model to the appropriate device\n",
        "criterion = nn.CrossEntropyLoss()  # Loss function for multi-class classification\n",
        "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)  # Adam optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGEtTK6XJxIJ"
      },
      "outputs": [],
      "source": [
        "# Training Function\n",
        "# Performs one epoch of training: forward pass, loss computation, backpropagation, and weight updates\n",
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss, epoch_acc = 0, 0  # Initialize epoch loss and accuracy\n",
        "    model.train()  # Set model to training mode\n",
        "    for batch in iterator:  # Iterate over training batches\n",
        "        inputs, labels = batch  # Unpack input data and labels\n",
        "        inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
        "        optimizer.zero_grad()  # Reset gradients\n",
        "        predictions = model(inputs)  # Forward pass: get predictions\n",
        "        loss = criterion(predictions, labels)  # Compute loss\n",
        "        acc = (predictions.argmax(1) == labels).float().mean()  # Compute accuracy\n",
        "        loss.backward()  # Backward pass: compute gradients\n",
        "        optimizer.step()  # Update model weights\n",
        "        epoch_loss += loss.item()  # Accumulate batch loss\n",
        "        epoch_acc += acc.item()  # Accumulate batch accuracy\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)  # Return average loss and accuracy\n",
        "\n",
        "# Evaluation Function\n",
        "# Evaluates the model on validation/test data without updating weights\n",
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss, epoch_acc = 0, 0  # Initialize epoch loss and accuracy\n",
        "    model.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
        "    with T.no_grad():  # Disable gradient computation for evaluation\n",
        "        for batch in iterator:  # Iterate over validation/test batches\n",
        "            inputs, labels = batch  # Unpack input data and labels\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to the appropriate device\n",
        "            predictions = model(inputs)  # Forward pass: get predictions\n",
        "            loss = criterion(predictions, labels)  # Compute loss\n",
        "            acc = (predictions.argmax(1) == labels).float().mean()  # Compute accuracy\n",
        "            epoch_loss += loss.item()  # Accumulate batch loss\n",
        "            epoch_acc += acc.item()  # Accumulate batch accuracy\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)  # Return average loss and accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBXY4sDxJzRO",
        "outputId": "40fb4b1e-6a2a-49ee-c46d-2634899fa605"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1:\n",
            "Train Loss: 0.5827, Train Accuracy: 0.6589\n",
            "Valid Loss: 0.4710, Valid Accuracy: 0.7711\n",
            "Epoch 2:\n",
            "Train Loss: 0.3275, Train Accuracy: 0.8609\n",
            "Valid Loss: 0.3604, Valid Accuracy: 0.8438\n",
            "Epoch 3:\n",
            "Train Loss: 0.1922, Train Accuracy: 0.9266\n",
            "Valid Loss: 0.3837, Valid Accuracy: 0.8470\n",
            "Epoch 4:\n",
            "Train Loss: 0.1105, Train Accuracy: 0.9594\n",
            "Valid Loss: 0.5561, Valid Accuracy: 0.8223\n",
            "Epoch 5:\n",
            "Train Loss: 0.0612, Train Accuracy: 0.9789\n",
            "Valid Loss: 0.5894, Valid Accuracy: 0.8294\n"
          ]
        }
      ],
      "source": [
        "# Training and evaluation loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}:\")\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
        "    print(f\"Valid Loss: {valid_loss:.4f}, Valid Accuracy: {valid_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiQEXerBJzdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70fa78d1-3b7e-4cfe-9785-d0be7fdbb3e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.5894, Test Accuracy: 0.8294\n"
          ]
        }
      ],
      "source": [
        "# Final evaluation on the test set\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Example sentiment mapping\n",
        "label_map = {0: \"negative\", 1: \"positive\"}  # Mapping model output to human-readable labels\n",
        "\n",
        "def classify_sentiment(model, tokenizer, input_text, device='cpu'):\n",
        "    \"\"\"\n",
        "    Classifies the sentiment of input_text using the trained model.\n",
        "    Args:\n",
        "        model: Trained transformer model.\n",
        "        tokenizer: Tokenizer function to preprocess input_text.\n",
        "        input_text: The text to classify.\n",
        "        device: 'cpu' or 'cuda' depending on your setup.\n",
        "    Returns:\n",
        "        Sentiment label (e.g., 'positive', 'negative').\n",
        "    \"\"\"\n",
        "    model.eval()  # Set the model to evaluation mode (disables dropout, etc.)\n",
        "    model.to(device)  # Move model to the specified device (CPU or GPU)\n",
        "\n",
        "    with torch.no_grad():  # Disable gradient computations for efficiency\n",
        "        # Tokenize and convert input to tensor\n",
        "        tokens = tokenizer(input_text)  # Tokenize the input text\n",
        "        input_ids = torch.tensor(tokens).unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
        "\n",
        "        # Pass input through the model to get logits (raw prediction scores)\n",
        "        logits = model(input_ids)\n",
        "\n",
        "        # Get the predicted label (index of the highest logit score)\n",
        "        predicted_label = torch.argmax(logits, dim=1).item()\n",
        "        return label_map[predicted_label]  # Map index to human-readable sentiment\n",
        "\n",
        "# Ensure correct device setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Automatically select GPU if available\n",
        "\n",
        "# Example usage-1\n",
        "example_text = \"The movie was absolutely fantastic! I loved it.\"  # Input text to classify\n",
        "sentiment = classify_sentiment(model, tokenizer, example_text, device=device)  # Get sentiment\n",
        "print(f\"Sentiment: {sentiment}\")  # Output the sentiment"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oiS9NWH4Hquj",
        "outputId": "65f9e1fc-89c9-41ff-9ca8-ab5046f2970e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage-2\n",
        "example_text = \"I hated the olympics\"\n",
        "sentiment = classify_sentiment(model, tokenizer, example_text, device=device)\n",
        "print(f\"Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8NN72rZyHq8a",
        "outputId": "89d26bdc-3777-4a4d-cce1-aefd751e8e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage-2\n",
        "example_text = \"My laptop is amazing\"\n",
        "sentiment = classify_sentiment(model, tokenizer, example_text, device=device)\n",
        "print(f\"Sentiment: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rmSOxCPKK6d",
        "outputId": "fc1e4d43-ef63-40e7-c0d4-8a6003102317"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentiment: positive\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}